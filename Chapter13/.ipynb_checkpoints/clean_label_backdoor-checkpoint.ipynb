{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Label Backdoor Attacks\n",
    "\n",
    "In this lab, we will use the ART library to simulate a clean label backdoor attack. Specifically, we are going to add small extra pattern to a subset of the training data and train a model that would falsely classify input when this extra patterns appears in the input data.\n",
    "\n",
    "We will be using the MNIST dataset for this exercise.\n",
    "\n",
    "The following code block import the necessary ART, tensorflow, and matplotlib packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import tensorflow.keras.backend as k\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.attacks.poisoning import PoisoningAttackBackdoor, PoisoningAttackCleanLabelBackdoor\n",
    "from art.attacks.poisoning.perturbations import add_pattern_bd\n",
    "from art.utils import load_mnist, preprocess, to_categorical\n",
    "from art.defences.trainer import AdversarialTrainerMadryPGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset\n",
    "\n",
    "In this code block, we load the MNIST dataset and randomly select 10000 data points from the MNIST data as target dataset to poison by adding a backdoor pattern\n",
    "\n",
    "The goal is to add a backdoor at test time of the model, and turn any digit with a backdoor pattern to a target digit, while all other digits without a backdoor predicted correctly.  \n",
    "\n",
    "This poisoning method is effective because before the trigger is added, a Project Gradient Descend (PGD) perturbation is added to the image based on a classifier the attacker trains separately. This makes the image difficult for the neural net to interpret, leaving only the backdoor trigger to use for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_raw, y_raw), (x_raw_test, y_raw_test), min_, max_ = load_mnist(raw=True)\n",
    "\n",
    "# Random Selection:\n",
    "n_train = np.shape(x_raw)[0]\n",
    "num_selection = 10000\n",
    "random_selection_indices = np.random.choice(n_train, num_selection)\n",
    "x_raw = x_raw[random_selection_indices]\n",
    "y_raw = y_raw[random_selection_indices]\n",
    "\n",
    "x_train, y_train = preprocess(x_raw, y_raw)\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "\n",
    "x_test, y_test = preprocess(x_raw_test, y_raw_test)\n",
    "x_test = np.expand_dims(x_test, axis=3)\n",
    "\n",
    "# Shuffle training data\n",
    "n_train = np.shape(y_train)[0]\n",
    "shuffled_indices = np.arange(n_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "x_train = x_train[shuffled_indices]\n",
    "y_train = y_train[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a neural network for the simulation\n",
    "\n",
    "We will build a simple CNN neural network for this simulation using the Keras framework. The network will take MNIST image as input and predict which digit shown in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Keras convolutional neural network - basic architecture from Keras examples\n",
    "# Source here: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "def create_model():    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a pattern for the backdoor\n",
    "\n",
    "To launch an attack, we need to create backdoor pattern. Here we use the PoisoningAttackBackdoor function and add_pattern_bd to create image pertubation with the pattern  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backdoor = PoisoningAttackBackdoor(add_pattern_bd)\n",
    "example_target = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
    "pdata, plabels = backdoor.poison(x_test, y=example_target)\n",
    "\n",
    "plt.imshow(pdata[1].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chose backdoor target labels for training\n",
    "\n",
    "This is a target attack, so we need to pick a target label for the model to predict when a backdoor is triggered.  Here we pick gigial 8. But you can pick any numner you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poison some percentage of all non-nines to nines\n",
    "targets = to_categorical([8], 10)[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Proxy Classifier\n",
    "\n",
    "Now we need to create posion data samples for the training. To do that, we first create a proxy model that is trained with Project Gradient Descent perturabltion.  We will then use this proxy model to generate the actual poison traing dataset in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(create_model())\n",
    "proxy = AdversarialTrainerMadryPGD(KerasClassifier(create_model()), nb_epochs=10, eps=0.15, eps_step=0.001)\n",
    "proxy.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the clean label poison data\n",
    "\n",
    "With the proxy model, we can now generate the clean label poison data for the attack later. Here we are going to poison 33% of the training data using the PoisoningAttackCleanLabelBackdoor funciton on the proxy model we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_poison = .33\n",
    "\n",
    "attack = PoisoningAttackCleanLabelBackdoor(backdoor=backdoor, proxy_classifier=proxy.get_classifier(),\n",
    "                                           target=targets, pp_poison=percent_poison, norm=2, eps=5,\n",
    "                                           eps_step=0.1, max_iter=200)\n",
    "pdata, plabels = attack.poison(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get modified data used to train the classifier\n",
    "\n",
    "Now we have the poisoned training dataset, we are not ready to train a actual normal model with this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "poisoned = pdata[np.all(plabels == targets, axis=1)]\n",
    "poisoned_labels = plabels[np.all(plabels == targets, axis=1)]\n",
    "model.fit(pdata, plabels, nb_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With the model trained. Let's see how the backdoor works\n",
    "\n",
    "But first, let's see how the model performs when we evaluate the model with normal clean test data (x_test).  As you can see the accuracy is very high (over 98%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on clean test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_preds = np.argmax(model.predict(x_test), axis=1)\n",
    "clean_correct = np.sum(clean_preds == np.argmax(y_test, axis=1))\n",
    "clean_total = y_test.shape[0]\n",
    "\n",
    "clean_acc = clean_correct / clean_total\n",
    "print(\"\\nClean test set accuracy: %.2f%%\" % (clean_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show prediction with clearn image\n",
    "\n",
    "Now let's take a look at a specific prediction example when the image is clean.  You can change the value of c and i to see the image and actual prediction.  As you can see the model predict clean sample correctly which is why the test evaluation is high and it can fool people that the model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = 0 # class to display\n",
    "i = 0 # image of the class to display\n",
    "\n",
    "c_idx = np.where(np.argmax(y_test, 1) == c)[0][i] # index of the image in clean arrays\n",
    "\n",
    "plt.imshow(x_test[c_idx].squeeze())\n",
    "plt.show()\n",
    "clean_label = c\n",
    "print(\"Prediction: \" + str(clean_preds[c_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attack with poisoned data\n",
    "\n",
    "Now let's see how the model behaves when we use a poisoned data with a back door.  As you can see, with poisoned data, the model will always output 8 as the prediction which is what we have designed the attack to be, and the model accuracy is 0% with poisoned dataset.\n",
    "\n",
    "You can change the value of c to try out different poisoned image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_target = np.logical_not(np.all(y_test == targets, axis=1))\n",
    "px_test, py_test = backdoor.poison(x_test[not_target], y_test[not_target])\n",
    "poison_preds = np.argmax(model.predict(px_test), axis=1)\n",
    "poison_correct = np.sum(poison_preds == np.argmax(y_test[not_target], axis=1))\n",
    "poison_total = poison_preds.shape[0]\n",
    "\n",
    "poison_acc = poison_correct / poison_total\n",
    "print(\"\\nPoison test set accuracy: %.2f%%\" % (poison_acc * 100))\n",
    "\n",
    "c = 1 # index to display\n",
    "plt.imshow(px_test[c].squeeze())\n",
    "plt.show()\n",
    "clean_label = c\n",
    "print(\"Prediction: \" + str(poison_preds[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
