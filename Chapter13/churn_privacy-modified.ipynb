{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nominated-navigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opacus\n",
      "  Downloading opacus-1.4.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.10/site-packages (from opacus) (1.26.3)\n",
      "Requirement already satisfied: torch>=2.0 in /opt/conda/lib/python3.10/site-packages (from opacus) (2.0.0.post200)\n",
      "Requirement already satisfied: scipy>=1.2 in /opt/conda/lib/python3.10/site-packages (from opacus) (1.11.4)\n",
      "Requirement already satisfied: opt-einsum>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from opacus) (3.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0->opacus) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n",
      "Downloading opacus-1.4.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opacus\n",
      "Successfully installed opacus-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elementary-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interested-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prime-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnDataset(Dataset):\n",
    " \n",
    "    def __init__(self, csv_file):\n",
    "  \n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        df = df.drop([\"Surname\", \"CustomerId\", \"RowNumber\"], axis=1)\n",
    "\n",
    "        # Grouping variable names\n",
    "        self.categorical = [\"Geography\", \"Gender\"]\n",
    "        self.target = \"Exited\"\n",
    "\n",
    "        # One-hot encoding of categorical variables\n",
    "        self.churn_frame = pd.get_dummies(df, prefix=self.categorical)\n",
    "\n",
    "        # Save target and predictors\n",
    "        self.X = self.churn_frame.drop(self.target, axis=1)\n",
    "        self.y = self.churn_frame[\"Exited\"]\n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_array  = scaler.fit_transform(self.X)\n",
    "        self.X = pd.DataFrame(X_array)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.churn_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return [self.X.iloc[idx].values, self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "joint-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CHURN_model():\n",
    "    model = nn.Sequential(nn.Linear(13, 64), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64, 64), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64, 1)) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "noble-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(csv_file, batch_size):\n",
    "     # Load dataset\n",
    "    dataset = ChurnDataset(csv_file)\n",
    "\n",
    "    # Split into training and test\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return trainloader, testloader, trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "breeding-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, net, optimizer, n_epochs=100):\n",
    "     \n",
    "    device = \"cpu\"\n",
    "\n",
    "    # Define the model\n",
    "    #net = get_CHURN_model()\n",
    "    net = net.to(device)\n",
    "    \n",
    "    #criterion = nn.CrossEntropyLoss() \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    # Train the net\n",
    "    loss_per_iter = []\n",
    "    loss_per_batch = []\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save loss to plot\n",
    "            running_loss += loss.item()\n",
    "            loss_per_iter.append(loss.item())\n",
    "\n",
    "        \n",
    "        print(\"Epoch {} - Training loss: {}\".format(epoch, running_loss/len(trainloader))) \n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cooked-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"data/churn.csv\"\n",
    "\n",
    "trainloader, testloader, train_ds, test_ds = get_dataloader(csv_file, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "traditional-physiology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.4377533576451242\n",
      "Epoch 1 - Training loss: 0.3663581402041018\n",
      "Epoch 2 - Training loss: 0.3442584626376629\n",
      "Epoch 3 - Training loss: 0.33838390540331603\n",
      "Epoch 4 - Training loss: 0.33441441049799325\n",
      "Epoch 5 - Training loss: 0.33233569730073215\n",
      "Epoch 6 - Training loss: 0.3310216405428946\n",
      "Epoch 7 - Training loss: 0.3281798171810806\n",
      "Epoch 8 - Training loss: 0.3269441844895482\n",
      "Epoch 9 - Training loss: 0.32509594233706596\n",
      "Epoch 10 - Training loss: 0.32279583225026726\n",
      "Epoch 11 - Training loss: 0.3208660346455872\n",
      "Epoch 12 - Training loss: 0.3186106376349926\n",
      "Epoch 13 - Training loss: 0.3177291705273092\n",
      "Epoch 14 - Training loss: 0.3174736758694053\n",
      "Epoch 15 - Training loss: 0.3157928120344877\n",
      "Epoch 16 - Training loss: 0.31431026444770394\n",
      "Epoch 17 - Training loss: 0.3124549970962107\n",
      "Epoch 18 - Training loss: 0.30975242527201774\n",
      "Epoch 19 - Training loss: 0.31037411466240883\n",
      "Epoch 20 - Training loss: 0.30951178735122087\n",
      "Epoch 21 - Training loss: 0.3084459109697491\n",
      "Epoch 22 - Training loss: 0.306449399003759\n",
      "Epoch 23 - Training loss: 0.3053556942380965\n",
      "Epoch 24 - Training loss: 0.30297225788235665\n",
      "Epoch 25 - Training loss: 0.30075222477316854\n",
      "Epoch 26 - Training loss: 0.29977624490857124\n",
      "Epoch 27 - Training loss: 0.2997215715702623\n",
      "Epoch 28 - Training loss: 0.29725136812776326\n",
      "Epoch 29 - Training loss: 0.294293462112546\n",
      "Epoch 30 - Training loss: 0.2935646055266261\n",
      "Epoch 31 - Training loss: 0.2914721766486764\n",
      "Epoch 32 - Training loss: 0.292428348492831\n",
      "Epoch 33 - Training loss: 0.28729050103574993\n",
      "Epoch 34 - Training loss: 0.28633431969210504\n",
      "Epoch 35 - Training loss: 0.2852407651487738\n",
      "Epoch 36 - Training loss: 0.2829290269408375\n",
      "Epoch 37 - Training loss: 0.2829291105736047\n",
      "Epoch 38 - Training loss: 0.279298481810838\n",
      "Epoch 39 - Training loss: 0.27780943219549953\n",
      "Epoch 40 - Training loss: 0.2767344255000353\n",
      "Epoch 41 - Training loss: 0.2747173799667507\n",
      "Epoch 42 - Training loss: 0.27302179504185914\n",
      "Epoch 43 - Training loss: 0.26763812666758896\n",
      "Epoch 44 - Training loss: 0.26990350764244797\n",
      "Epoch 45 - Training loss: 0.26652999366633595\n",
      "Epoch 46 - Training loss: 0.26774101946502926\n",
      "Epoch 47 - Training loss: 0.2652529938146472\n",
      "Epoch 48 - Training loss: 0.2612146223895252\n",
      "Epoch 49 - Training loss: 0.26180493235588076\n"
     ]
    }
   ],
   "source": [
    "net = get_CHURN_model()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.0001, lr=0.003)\n",
    "\n",
    "model = train(trainloader, net, optimizer, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "valuable-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_per_sample_grad_norm = 1.5\n",
    "sample_rate = batch_size/len(train_ds)\n",
    "noise_multiplier = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "concerned-directory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.5341288127936423\n",
      "Epoch 1 - Training loss: 0.5202931649051606\n",
      "Epoch 2 - Training loss: 0.5341383019695058\n",
      "Epoch 3 - Training loss: 0.5288592478260398\n",
      "Epoch 4 - Training loss: 0.5171439621597529\n",
      "Epoch 5 - Training loss: 0.5163358460646122\n",
      "Epoch 6 - Training loss: 0.5369994322769344\n",
      "Epoch 7 - Training loss: 0.49765840321779253\n",
      "Epoch 8 - Training loss: 0.5046963236760348\n",
      "Epoch 9 - Training loss: 0.5088824790902435\n",
      "Epoch 10 - Training loss: 0.5092353354673833\n",
      "Epoch 11 - Training loss: 0.5087992254644632\n",
      "Epoch 12 - Training loss: 0.5309229969978333\n",
      "Epoch 13 - Training loss: 0.5106239337474108\n",
      "Epoch 14 - Training loss: 0.5036877773702144\n",
      "Epoch 15 - Training loss: 0.4927969908807427\n",
      "Epoch 16 - Training loss: 0.48599050249904396\n",
      "Epoch 17 - Training loss: 0.5188661337830126\n",
      "Epoch 18 - Training loss: 0.5169508713996038\n",
      "Epoch 19 - Training loss: 0.49798491708934306\n",
      "Epoch 20 - Training loss: 0.503883030358702\n",
      "Epoch 21 - Training loss: 0.5164223099593073\n",
      "Epoch 22 - Training loss: 0.5056664399802685\n",
      "Epoch 23 - Training loss: 0.5065449120476841\n",
      "Epoch 24 - Training loss: 0.5433282388490625\n",
      "Epoch 25 - Training loss: 0.5174264007247984\n",
      "Epoch 26 - Training loss: 0.4976155414246023\n",
      "Epoch 27 - Training loss: 0.5491775732953101\n",
      "Epoch 28 - Training loss: 0.5384174400009215\n",
      "Epoch 29 - Training loss: 0.540379109699279\n",
      "Epoch 30 - Training loss: 0.5356422456097789\n",
      "Epoch 31 - Training loss: 0.526036859350279\n",
      "Epoch 32 - Training loss: 0.5539815332740545\n",
      "Epoch 33 - Training loss: 0.5018121126107872\n",
      "Epoch 34 - Training loss: 0.5338452438823879\n",
      "Epoch 35 - Training loss: 0.5484621480689384\n",
      "Epoch 36 - Training loss: 0.5306439772713929\n",
      "Epoch 37 - Training loss: 0.5249972022138536\n",
      "Epoch 38 - Training loss: 0.5311912927776575\n",
      "Epoch 39 - Training loss: 0.5432186192367225\n",
      "Epoch 40 - Training loss: 0.5510282004252076\n",
      "Epoch 41 - Training loss: 0.5241838982328773\n",
      "Epoch 42 - Training loss: 0.5489578522741795\n",
      "Epoch 43 - Training loss: 0.5351523550227284\n",
      "Epoch 44 - Training loss: 0.5623936931602657\n",
      "Epoch 45 - Training loss: 0.5204690810758621\n",
      "Epoch 46 - Training loss: 0.5256892706267535\n",
      "Epoch 47 - Training loss: 0.5685370249673725\n",
      "Epoch 48 - Training loss: 0.5429427841212601\n",
      "Epoch 49 - Training loss: 0.5486146748065949\n"
     ]
    }
   ],
   "source": [
    "from opacus import PrivacyEngine\n",
    "net = get_CHURN_model()\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.0001, lr=0.003)\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, dataloader = privacy_engine.make_private(\n",
    "    module=net,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    max_grad_norm=max_per_sample_grad_norm,\n",
    "    optimizer = optimizer,\n",
    "    data_loader = trainloader)\n",
    "model = train(dataloader, model, optimizer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "choice-picnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ε = 5.19\n"
     ]
    }
   ],
   "source": [
    "epsilon = privacy_engine.accountant.get_epsilon(delta=1e-5)\n",
    "print (f\" ε = {epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca04f09-21fd-428a-929e-83015030b380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
